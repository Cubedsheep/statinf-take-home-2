\documentclass[a4paper]{article}
\setlength{\parskip}{11pt plus 1pt minus 1pt}

%% Language and font encodings
\usepackage[dutch]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[backend=bibtex]{biblatex}
\usepackage{url}
% \addbibresource{bibliografie.bib}
\usepackage[usenames,dvipsnames]{color}


%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=1in]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[dutch]{babel}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=false, allcolors=blue]{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{siunitx}
\usepackage[backend=bibtex]{biblatex}
\usepackage{subcaption}
\usepackage{gensymb}

%% tikz for plots
\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=plots/]
% package to determine line width
\usepackage{layouts}

\newcommand{\R}{\mathbb{R}}

\title{Take home 2}
\author{Pieter Luyten}

\begin{document}

\maketitle

\section*{Excercise 1}
\subsection*{(a)}
\noindent \fbox{Fit a linear model, assuming that the strong Gaussian assumption is relevant.}

The value for the intercept of the fit is $1.0321764$ and for the rico of the fit is $0.1904323$
\begin{figure}[h]
	\centering
	\input{fit_1a.tex}
	\caption{line fit throught the data in Ex1.txt}
	\label{fig:fit-1a}
\end{figure}

\subsection*{(b)}
\noindent \fbox{Test whether $\beta_1=0$ at the level $\alpha=0.01$.}

Using the result from section 7.4.1 in <REFERENCE cursus> we know that the random variable
\begin{equation}
	T = \frac{\beta_1}{\sqrt{ \frac{S^2}{ \sum_{i=1}^{n}(x_i-\bar{x}^2)}}}
\end{equation}
has a Student distribution with $n-2$ degrees of freedom. The test value is $2.603$. Using a student-t distribution with $60-2=58$ degrees of freedom we find a p-value of $0.0117$. At the confidence level $\alpha=0.01$, the null hypothesis that $\beta_1=0$ holds. The $99\%$ confidence region for the test value is $[-2.663, 2.663]$.

\subsection*{(c)}
\noindent \fbox{Explain what a Q-Q plt is and apply this to the residuals.}

A q-q plot is a plot where the quantiles of the assumed distribution are plotted against the quantiles from the sample. So in a sample of $n$ points where we label the observation $x_i, i \in \{1,2, \ldots, n\}$ from lowest to highest, the ith point will be plotted at the $(Q(i/n), x_i)$. 
Here $Q(x)$ is the quantile function of the assumed distribution. This is a function such that $ P( X < Q(p)) = p$. If the assumed distribution is a good model for the observed sample, the points will well fitted by a linear function.

\begin{figure}[h]
	\centering
	\input{qq-plot.tex}
	\caption{qq plot for the errors on the fit}
	\label{fig:qqplot-1c}
\end{figure}

\subsection*{(d)}
\noindent \fbox{Perform a test that does not require normality of the errors.}

To do a test that does not require normality of the errors we first derive an assymptotic result for the distribution of $\hat{\beta}$.
We use that $\hat{\beta}$ is given by:
\begin{equation}
	\hat{\beta} = (X^TX)^{-1} X^TY
	\label{eq:beta_hat}
\end{equation}
And that we can rewrite $Y$ as $Y=X\beta + \epsilon$ where $\epsilon$ is the random vector of errors. Filling this in in equation \ref{eq:beta_hat} yields
\begin{align*}
	\hat{\beta} &= (X^TX)^{-1} X^T (X\beta + \epsilon)\\
		    &= (X^TX)^{-1} (X^TX) \beta + (X^TX)^{-1}X^T\epsilon\\
		    &= \beta + (X^TX)^{-1}X^T\epsilon
\end{align*}
We see that the distribution of $\hat{\beta}$ is a linear combination of the distributions of the individual errors.
\begin{equation}
	\hat{\beta}_i = \beta_i + \sum_{j=1}^{n}[(X^TX)^{-1}X^T]_{ij} \epsilon_j
\end{equation}
Let $A = (X^TX)^{-1}X$. If we assume that the $\epsilon_i$ are i.i.d. with $E(\epsilon_i)=0$ and $Var(\epsilon_i)=\sigma^2$ we see that the distribution of $\hat{\beta}_i$ has expectation value $E(\hat{\beta}_i)=\beta_i$ and $Var(\hat{\beta}_i) = (A_i \cdot A_i^T) \sigma^2$. 

\subsection*{(e)}
\noindent \fbox{Determine a $99\%$ confidence region for $\hat{\beta}$}

\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{region-1e.pdf}
	\caption{A plot of the $99\%$ confidence region for $\beta$.}
	\label{fig:1e}
\end{figure}
We use the formula from section 7.5.4 in the book to find a confidence region for $\mathbf{\beta}$. THis region is of the form:
\begin{equation}
	\left\{ \beta \bigg| \frac{(\hat{\beta}-\beta)^T (X^TX) (\hat{\beta}-\beta)}{pS^2} \leq F_{p,n-p}(1-\alpha) \right\}
\end{equation}
Because the matrix $X^TX$ is positive definite, this is an ellips with center $\mathbf{\beta}$ and axes along the eigenvectors of $X^TX$. The upper bound for this expression is in this case: $F_{2,58(0.99) = 4.9910}$ Filling in the values for $X, p=2, n=60$ and $S^2$ we get the following result (obtained using the cAS system sympy):
\begin{equation*}
	\{ \beta | 5.2596 \alpha^{2} + 0.8222 \alpha \beta - 0.4950 \alpha + 1.6711 \beta^{2} - 0.6629 \beta + 0.0711 \leq 4.9910 \}
\end{equation*}
The resulting ellips is plotted in figure \ref{fig:1e}.


\newpage
\section*{Question 2}

\subsection*{(a)}
\noindent \fbox{Compute the ordinary least squares $\hat{\beta}_{OLS}$.}

The value for $\hat{\beta}_{OLS}$ obtained using ordinary least squares with the correlated errors is:
\begin{equation}
	\hat{\beta}_{OLS} = (3.923161, 1.214355)^T
\end{equation}

\subsection*{(b)}
\noindent \fbox{\parbox{0.8\linewidth}{Suppose we further know that the errors are correlated and satisfy the following equation:
	\begin{equation}
		\epsilon_i = \rho \epsilon_{i-1} + \eta_i, \quad \text{for} i = 1,\ldots, n
		\label{eq:errors}
	\end{equation}
Wwhere $\eta_n$ are i.i.d. standar Gaussian and $\rho=0.8$. Use \ref{eq:errors} to compute the variance of $\mathbf{\epsilon}$.}}

we know that the variance is the same for all $\epsilon_i$ and that they obey the recursive relation:
\begin{equation*}
	\epsilon_i = \rho \epsilon_{i-1} + \eta_i
\end{equation*}
with $\eta_i$ standard normaly distributed. Using $Var(\epsilon_i) =Var(\epsilon_j)$ for all $i, j \in \{1, 2, \ldots 80\}$ we find:
\begin{align*}
	Var(\epsilon_i) &= \rho^2 Var(\epsilon_{i-1}) + Var(\eta_i)\\
	\Leftrightarrow Var(\epsilon_i) &= \rho^2 Var(\epsilon_i) + 1\\
	\Leftrightarrow Var(\epsilon_i) &= \frac{1}{1-\rho^2}\\
					&= \frac{25}{9}\\
					&= 2.777 \ldots
\end{align*}
We conclude that the variance of $\epsilon$ is $ \frac{25}{9}$.

\subsection*{(c)}
\noindent \fbox{\parbox{\linewidth}{Transform the model in such a way that the errors are no longer correlated. cCompute the ordinary least squares for this new model and compare it to the one obtained in (a)}}

To transform the errors to a basis where they are no longer correlated we will use the variance-covariance matrix $V$ of the errors. 
The off-diagonal elements, so the covariances, are (suppose $i < j$:
\begin{align*}
	Cov(\epsilon_i, \epsilon_j) &= E[(\epsilon_i-E(\epsilon_i))(\epsilon_j-E(epsilon_j))]\\
				    &= E[\epsilon_i \epsilon_j]\\
				    &= E[\epsilon_i (\rho\epsilon_{j-1} \eta_j)]\\
				    &\vdots\\
				    &= E[\epsilon_i (\rho^{j-i} \epsilon_i + \rho^{j-i-1}\eta_i + \rho^{j-i-2}\eta_{i-2} + \ldots + \eta_j)]\\
				    &= E[\epsilon_i \rho^{i-j} \epsilon_i]\\
				    &= \rho^{i-j} Var(\epsilon)\\
				    &= \rho^{i-j} \frac{25}{9}
\end{align*}
So the matrix V has $ \frac{25}{9}$ on the diagonal, $ \frac{25}{9}\rho $ on the two sub-diagonals next to the diagonal, $ \frac{25}{9} \rho^k$ on the $k$th sub-diagonal.
the matrix $V$ can be diagonalized with an orthogonal (or unitary) transformation $U$: $\Lambda = MVM^T$. When we use the definition of the variance-covariance matrix (where $\mathbf{\epsilon}$ denotes the column vector with the errors) we find:
\begin{align*}
	V &= E( (\mathbf{\epsilon} - E(\mathbf{\epsilon})) (\mathbf{\epsilon} - E(\mathbf{\epsilon}))^T)\\
	\Leftrightarrow \Lambda &= U E( \mathbf{\epsilon} \mathbf{\epsilon}^T) U^T\\
	\Leftrightarrow \Lambda &= E( (U\mathbf{\epsilon})(U\mathbf{\epsilon})^T)\\
\end{align*}
Where we used that $E(\mathbf{\epsilon} = \mathbf{0})$. The vector $U \mathbf{\epsilon}$ has variance-covariance matrix $\Lambda$ so we conclude that it is a vector of uncorrelated stochastic variables.

The value for $\hat{\beta}_{OLS}$ that we find using these uncorrelated errors is exactly the same as with the uncorrelated errors:
\begin{equation*}
	\hat{\beta}_{OLS} = (3.923161, 1.214355)^T
\end{equation*}
This is expected because we used a unitary transformation to get uncorrelated errors and by filling this in the expression for the Sum of squares of the remainders we find:
\begin{align*}
	RSS(\hat{\beta}_{Uncorrelated}) &= (UY-UX\beta)^T(UY-UX\beta)\\
		   &= (Y-X\beta)^T U^T U (Y-X\beta)\\
		   &= (Y-X\beta)^T I (Y-X\beta)\\
		   &= RSS(\hat{\beta}_{Correlated})
\end{align*}
which means the RSS for the correlated and the uncorrelated errors will have the same optimal $\hat{\beta}$.


\newpage
\section*{Question 3}

\subsection*{(a)}
\noindent \fbox{Comput the weighted least square estimator $\hat{\beta}_{WLS}$}

The values for the fitted coefficients with ordinary least squares are: $[0.4994, 1.1985, -2.4959, 1.2082]$. In figure \ref{fig:fit-3} the fit is plotted along with the data. 
\begin{figure}
	\centering
	\input{fit_3.tex}
	\caption{fits of a cubic function using ordinary least square (blue) and weighted least squares (red)}
	\label{fig:fit-3}
\end{figure}

\subsection*{(b)}
\noindent \fbox{\parbox{\linewidth}{Suppose we further know that the errors are Gaussian but heteroscedastic as follows:
\begin{equation*}
	\sigma(x) = \bigg\{ \begin{matrix}x^2 & \text{if} x \in [0,4/3]\\ 4(x-2)^2 & \text{if} x \in [4/3, 2] \end{matrix}
\end{equation*}
Compute the weighted least square estimator $\hat{\beta}_{WLS}$.}}

The coeffitients fitted with the weighted least square algorithm are $[0.5003, 0.9669, -1.9655, 0.9910]$.

\subsection*{(c)}
\noindent \fbox{Compare $\hat{\beta}_{WLS}$ to $\hat{\beta}_{OLS}$ and with the true parameter i.e. $\beta=(0.5,1,-2,1)^T$ }

The coefficients that are calculated using the least squares method are closer to the real values than the ones calculated using the ordinary least squares. To really say something about the difference and how well they match the given real values of the function we need more information about the distribution of the parameters.
In figure \ref{fig:fit-3} the two fits are plotted together with the data to fit through.

\subsection*{(d)}
\noindent \fbox{Determine the distribution of $\hat{\beta}_{WLS}$}

To derive the distribution of $\hat{\beta}_{WLS}$ we start from the distribution for the errors, which is known:
\begin{equation*}
	\mathbf{\epsilon} \sim N_n(0, W)
\end{equation*}
Where $W$ is a diagonal matrix with at position $W_{ii}$ the variance of $\epsilon_i$. This is the same matrix $W$ used in the formula to calculate the estimate for $\beta$ with the weighted least squares method. This formula is:
\begin{equation}
	\hat{\beta}_{WLS} = (X^TW^{-1}X)^{-1} X^T W^{-1}Y
	\label{eq:beta_WLS}
\end{equation}
here $Y$ is the vector with samples used to calculate the least squares estimator. $X$ is the matrix as defined on page 193 in the course notes, this matrix is deterministic. $Y$ can be rewritten as follows:
\begin{equation*}
	Y = X \beta + \mathbf{\epsilon}
\end{equation*}
substituting this into equation \ref{eq:beta_WLS} yields:
\begin{align*}
	\hat{\beta}_{WLS} &= (X^TW^{-1}X)^{-1} X^TW^{-1}(X\beta+\mathbf{\epsilon})\\
			  &= (X^TW^{-1}X)^{-1} (X^TW^{-1}X) \beta + (X^TW^{-1}X)^{-1} X^TW^{-1}\mathbf{\epsilon}\\
			  &= \beta + X^{-1} W X^{-T} X^T W^{-1} \mathbf{\epsilon}\\
			  &= \beta + X^{-1} \mathbf{\epsilon} 
\end{align*}
We see this is a linear transformation of a multivariate normal distribution with mean $\mathbf{0}$ and as variance-covariance matrix $\Sigma_{\epsilon}=W$. Again using the results from section 6.2, page 172 in the course notes we find that the distribution of $\hat{\beta}_{WLS}$ is a multivariate normal distribution with mean $\beta$ and variance-covariance matrix $X^{-1} W X^{-T} = (X^T W^{-1} X)^{-1}$. We conclude that:
\begin{equation*}
	\hat{\beta}_{WLS} \sim N_p(\beta, (X^TW^{-1}X)^{-1})
\end{equation*}


\newpage
\section*{Question 4}
\noindent \fbox{\parbox{\linewidth}{Let $\mathbf{X}$ be a 3-dimensional Gaussian vector with parameters
\begin{align*}
	\mu = \begin{pmatrix}1\\1\\-2\end{pmatrix} \quad &\text{and} \quad \Sigma = \begin{pmatrix}1.25&1.50&0.5\\1.5&5.25&3.5\\0.5&3.5&3.0\end{pmatrix}
\end{align*}}}

\subsection*{(a)}
\noindent \fbox{Produce $n=200$ simulations of $\mathbf{X}$}

To generate a sample from a multi-normal distribution we work analogues to the derivation in section 6.2 in the course notes, but backwards. We are given the variance-covariance matrix $\Sigma_X$ of a multivariate normal model. This matrix must be positive definite, so the square root of this matrix exists. 
Let $A$ be a matrix such that $AA^T=\Sigma_X$. This matrix can be found by diagonalizing $\Sigma_X$: $\Sigma_X=U\Lambda U^T$ and than taking $A = U \sqrt(\Lambda)$. We can do this because all eigenvalues of $\Sigma_X$ are positive.
The result from section 6.2 now tells us that $X = AY + \mu$ where $Y$ is a vector of standard normal distributions.
This observation can be used to draw a sample from $X$ by drawing $n$ independent samples from a standard normal distribution, filling them in in a vector $y$ and the calculating the sample from $X$ as $x=Ay+\mu$. This was implemented in $R$

\begin{figure}
	\centering
	\input{sample-4a-xy.tex}
	\caption{Projection on the $xy$-plane of the sample from the multivariate normal distribution}
	\label{fig:sample}
\end{figure}

\subsection*{(b)}
\noindent \fbox{Compute $P(X_1>1|X_2=1, X_3=-2)$ and $P(X_1>1|X_2+X_3=-1)$.}

To compute these conditional chances we use proposition 6.2 from the course notes with the following vectors and matrices:
\begin{align*}
	\Sigma_{11} &= \left(1.25\right) &\Sigma_{12} = \left( 1.50, 0.5\right)\\
	\Sigma_{21} &= \begin{pmatrix} 1.50\\ 0.50 \end{pmatrix} &\Sigma_{22} = \begin{pmatrix} 5.25 & 3.5\\ 3.5 & 3.0 \end{pmatrix}\\
	\mathbf{X}_1 &= \begin{pmatrix} \mathbf{X}_1 \end{pmatrix} &\mathbf{x}_2 = \begin{pmatrix} 1\\-2 \end{pmatrix}
\end{align*}
When we fill in these values in the the formula for the distribution of the conditional distribution $f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1, \mathbf{X}_2 = \mathbf{x}_2)$ we find the following distribution:
\begin{equation}
	f_{\mathbf{X}_1|\mathbf{X}_2}(\mathbf{x}_1, \mathbf{X}_2 = \mathbf{x}_2) = N_1(1, 0.446)
\end{equation}
The probability $P(X_1>1|X_2=1, X_3=-2)$ is equal to $0.5$ because of the symetry of the conditional probability distribution around $1$.

To calculate the probability $P(X_1 > 1|X_2+X_3=1)$ we first transform the random vector $\mathbf{X}$ to the vector $\mathbf{Y} = (X_1, X_2+X_3)^T=\mathbf{A}\mathbf{X}$ where
\begin{equation*}
	\mathbf{A} = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 1 \end{pmatrix}
\end{equation*}
Using part (b) of proposition 6.2 we find that the distribution of $\mathbf{Y}$ is given by:
\begin{equation*}
	\mathbf{Y} \sim N(A\mathbf{\mu}, \mathbf{A}\Sigma\mathbf{A}^T) = N(...)
\end{equation*}
Observe that $P(X_1 > 1 | X_2+X_3=-1) = P(Y_1 > 1 | Y_2=-1)$. We can again use part (c) of proposition 6.2 to calculate this probability. This yields the following conditional probability distribution:
\begin{equation*}
	f_{\mathbf{Y}_1|\mathbf{Y}_2}(\mathbf{y}_1|\mathbf{y}_2=-1) = N(1, 0.9877)
\end{equation*}
Therefore $P(X_1 > 1 | X_2+X_3=-1)=0.5$.

\subsection*{(c)}
\noindent \fbox{\parbox{\linewidth}{Let $\mathbf{Y}=(X_1, X_2)^T$. Represent graphically the density contours that comprise $95\%$ of the probability mass of $\mathbf{Y}$}}

To determine the density function of $\mathbf{Y}=(X_1, X_2)^T$ We make again use of proposition 6.2 (c) with the following matrix:
\begin{equation*}
	\mathbf{A} = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0 \end{pmatrix}
\end{equation*}
From which we find the probability distribution of $\mathbf{Y}$
\begin{equation*}
	\mathbf{Y} \sim N_2\left( \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix}1.25&1.50\\1.50&5.25 \end{pmatrix}\right)
\end{equation*}
We can now use proposition 6.1 to find the contours of of this density function. The contour that comprises $95\%$ of the probability mass is given by the following equation:
\begin{align*}
	Q_{\chi^2_2}(0.95) &= (\mathbf{Y}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{Y}-\mathbf{\mu})
\end{align*}
This region is plotted in figure
\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{region-3c.pdf}
	\caption{The contour that comprises $95\%$ of the density of the probability density function of the random vector $\mathbf{Y}$.}
	\label{fig:4c}
\end{figure}

\section*{Acknowledgements}
Rune Buckinx and MichaÃ«l Maex for helping to fix stupid mistakes in stupid R code
Seppe for stupid discussions about how well the questions are asked
Thibeau for organising a group crying session
Robbe for genuinly good input about trying to solve stupid questions


%\printinunitsof{in}\prntlen{\textwidth}


\end{document}
